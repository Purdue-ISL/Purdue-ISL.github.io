import{_ as h}from"./HeaderNav.31d35145.js";import{_ as u}from"./PageHeader.16da4edc.js";import{_ as v}from"./SubPageNav.53e681b8.js";import{_ as g,a as m}from"./separator.e97cfb2e.js";import{_ as f,c as _,a as e,b as i,d as o,f as b,o as y}from"./entry.b2a63bb5.js";import"./nuxt-link.bcb6b7be.js";import"./_commonjsHelpers.725317a4.js";let n=[];const w={beforeRouteEnter(s,t,a){console.log(t),console.log(s),s.params.link=t,a()},methods:{overlap(s,t){return(window.screen.height-s.getBoundingClientRect().top)/window.screen.height>t},handleScroll(){var s=[];for(var t in n)this.overlap(n[t],.1)&&(n[t].classList.toggle("show-up"),s.push(t));for(var t in s)delete n[s[t]];Object.keys(n).length==0&&(console.log("removed"),window.removeEventListener("scroll",this.handleScroll))}},mounted(){window.scrollTo(0,0),n=[],document.querySelectorAll(".subpages-section-container-div").forEach(t=>{n.push(t),t.classList.contains("show-up")&&t.classList.toggle("show-up")}),window.addEventListener("scroll",this.handleScroll),window.addEventListener("load",this.handleScroll),this.handleScroll()}},x={id:"doc",class:"bg-[var(--primary-color)] bg-img min-h-screen"},P={class:"limit-width"},S={class:"limit-width"},k=e("div",{class:"w-[96%] mx-auto"},null,-1),C=e("h1",{class:"subpages-title"}," Next Generation Multi-Perspective Video Delivery at Internet Scale ",-1),D=e("div",{id:"des",class:"subpages-section-container-div"},[e("div",{class:"subpages-section-title-container-div"},[e("h1",{class:"subpages-section-title"},"Description")]),e("div",{class:"subpages-section-content"},[e("p",null,[o(" The success of streaming video has generated interest in newer forms of multi-perspective video content, such as those generated by 360-degree cameras, multi-angle camera arrays, or light-field cameras. The immersive experience provided by these cameras can enhance user satisfaction in domains including sports, training (e.g., construction safety), and virtual exploration (e.g., college walkthroughs, historical sites). With content from these cameras, users do not just passively consume content, but may interactively traverse the content along many different paths from a perspective of their choice, with different users observing different perspectives of the same content. To support this at Internet-scale is challenging; client players must be able to switch perspectives with low latency, the perspectives may need to be generated on demand by video servers, and the infrastructure must support a variety of devices for capture and consumption of this content. "),e("br"),e("br"),o(" This project explores architectural enhancements, algorithms, and techniques to deliver multi-perspective video at Internet-scale. It couples delivery optimization with video coding and human computer interaction. The project will develop interactivity abstractions by which content publishers can specify the range of perspectives users are permitted to choose from at each point in the video. The interactivity specified in a video will (i) drive perspective coding and novel dynamic perspective generation algorithms; (ii) enable infrastructure provisioning to meet Content Delivery Network (CDN) storage and cost constraints; and (iii) will guide adaptive perspective retrieval from CDN servers or from nearby caches. Finally, the project will explore methods to predict and guide user behavior to improve delivery quality based on user studies. "),e("br"),e("br")])])],-1),j={id:"pub",class:"subpages-section-container-div"},N=e("div",{class:"subpages-section-title-container-div"},[e("h1",{class:"subpages-section-title"},"Publications")],-1),T={class:"subpages-section-content"},V=b('<div id="ppl" class="subpages-section-container-div"><div class="subpages-section-title-container-div"><h1 class="subpages-section-title">Team</h1></div><div class="subpages-section-content"><div><span class="font-bold text-xl">Faculty</span><div class="text-lg pl-2"><ul class="gap-10"><li class="font-mono tracking-tighter">Prof. Sanjay Rao</li></ul></div></div><br><div><span class="font-bold text-xl">Students</span><div class="text-lg pl-2"><ul class="gap-10"><li><span class="font-mono tracking-tighter"> Ehab Ghabashneh </span><span>— Ph.D. student</span></li><li><span class="font-mono tracking-tighter"> Chandan Bothra </span><span> — Ph.D. student</span></li><li><span class="font-mono tracking-tighter"> Xinji (Jimmy) Jiang </span><span> — Ph.D. student</span></li><li><span class="font-mono tracking-tighter"> Clayton Walker</span><span> — Undergraduate researcher</span></li><li><span class="font-mono tracking-tighter"> Jagan Krishnasamy</span><span> — Undergraduate researcher</span></li><li><span class="font-mono tracking-tighter"> Anjali Vanamala</span><span> — Undergraduate researcher</span></li></ul></div></div><br><div><span class="font-bold text-xl">Collaborators</span><div class="text-lg pl-2"><ul class="gap-10"><li><span class="font-mono tracking-tighter">Prof. Ramesh Govindan</span><span> — University of Southern California</span></li><li><span class="font-mono tracking-tighter">Prof. Antonio Ortega</span><span> — University of Southern California</span></li><li><span class="font-mono tracking-tighter">Prof. Alex Quinn</span><span> — Purdue University</span></li></ul></div></div><br></div></div>',1),E=e("div",{class:"pb-[30px]"},null,-1);function G(s,t,a,B,I,L){const r=h,c=u,l=v,p=g,d=m;return y(),_("div",x,[e("div",P,[i(r)]),i(c,{title:"Projects"}),e("div",S,[k,C,i(l,{data:[["Description","des"],["Publications","pub"],["People","ppl"]]}),D,e("div",j,[N,e("div",T,[i(p,{title:"Dragonfly: Higher Perceptual Quality For Continuous 360° Video Playback",authors:"Ehab Ghabashneh, Chandan Bothra, Ramesh Govindan, Antonio Ortega, and Sanjay Rao",conference:"Proceedings of ACM Special Interest Group on Data Communications (SIGCOMM), 2023",paper:"../papers-pdf/dfly.pdf",slides:"../papers-pdf/Dragonfly-Sigcomm23_final.pptx",video:"https://www.youtube.com/watch?v=FHyaGPMBV6c",github:"https://github.com/Purdue-ISL/Dragonfly"}),i(d)])]),V]),E])}const H=f(w,[["render",G]]);export{H as default};
